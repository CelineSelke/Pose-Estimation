{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('../coco2014/annotations/person_keypoints_train2014.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movenet = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/singlepose-lightning/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_full_body(ann, min_keypoints=17):\n",
    "    keypoints = ann['keypoints']\n",
    "    num_visible = sum(1 for i in range(0, len(keypoints)) if keypoints[i] > 0)  # Count visible keypoints\n",
    "    if num_visible < min_keypoints:\n",
    "        return False  # Not all keypoints are visible\n",
    "    \n",
    "    bbox = ann['bbox']\n",
    "    bbox_aspect_ratio = bbox[3] / bbox[2]  \n",
    "    if bbox_aspect_ratio < 1.5:  # Threshold for a standing full-body shot\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../coco2014/images/train2014\"\n",
    "\n",
    "human_images_anns = []\n",
    "\n",
    "human_image_ids = set()\n",
    "for ann in coco.anns.values():\n",
    "    if ann['category_id'] == 1 and ann['iscrowd'] == 0 and is_full_body(ann,17) == True: #human, not a crowd, displays full body\n",
    "        human_image_ids.add(ann['image_id'])\n",
    "        human_images_anns.append(ann)\n",
    "count = 0\n",
    "human_image_paths = []\n",
    "for img_id in human_image_ids:\n",
    "    if count < 1000:\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{image_dir}/{img_info['file_name']}\"\n",
    "        human_image_paths.append(img_path)\n",
    "        count+=1\n",
    "\n",
    "print(len(human_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',\n",
    "                  'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "                  'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "connections = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "               (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "def detect_pose_static(image_path, ann):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    bbox = [a[\"bbox\"] for a in ann]\n",
    "    bbox = (bbox[0])\n",
    "    if (len(bbox) >= 4):\n",
    "        x_min, y_min, width, height = bbox[:4]\n",
    "        x_min = int(x_min)\n",
    "        y_min = int(y_min)\n",
    "        height = int(height)\n",
    "        width = int(width)\n",
    "        \n",
    "        # raises value error if bounding box dimensions exceed image dimensions \n",
    "        if x_min < 0 or y_min < 0 or (x_min+width) > image.shape[1] or (y_min+height) > image.shape[0]:\n",
    "            print(image_path)\n",
    "            raise ValueError(f\"Invalid bounding box: {x_min}, {y_min}, {x_min+width}, {y_min+height}\")\n",
    "        \n",
    "    # defaults to full image size in cases where bounding box does not meet requisite length \n",
    "    else:\n",
    "        x_min, y_min = 0,0\n",
    "        x_max, y_max = image.shape[1], image.shape[0]\n",
    "\n",
    "    # convert BGR image to HSI and split h, s, and i channels \n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    # Equalize intensity channel's histogram and merge channels back into singular image \n",
    "    v = cv2.equalizeHist(v)\n",
    "    hsv_image = cv2.merge([h, s, v])\n",
    "\n",
    "    # Converting from HSI to RGB, which MoveNet requires \n",
    "    final_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    # Cropping Image to first bounding box dimensions found in annotation for ease of prediction \n",
    "    cropped_image = final_image[y_min:y_min+height, x_min:x_min+width,:]   \n",
    "\n",
    "    #resizing image to match dimensions expected by MoveNet, followed by predicting keypoints \n",
    "    image_resized = tf.image.resize_with_pad(tf.expand_dims(cropped_image, axis=0), 192, 192) #192 for lightning\n",
    "    image_np = image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(image_np))\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    for i in range (0,17):\n",
    "        keypoints[0][0][i][1] = keypoints[0][0][i][1] * width + x_min\n",
    "        keypoints[0][0][i][0] = keypoints[0][0][i][0] * height + y_min\n",
    "\n",
    "    # Repeating with the un-equalized image for later comparison \n",
    "    cropped_image = image[y_min:y_min+height, x_min:x_min+width,:]    \n",
    "    original_image_resized = tf.image.resize_with_pad(tf.expand_dims(image, axis=0), 192, 192)\n",
    "    original_image_np = original_image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(original_image_np))\n",
    "    original_keypoints = outputs['output_0'].numpy()\n",
    "    for i in range (0,17):\n",
    "        original_keypoints[0][0][i][1] = original_keypoints[0][0][i][1] * width + x_min\n",
    "        original_keypoints[0][0][i][0] = original_keypoints[0][0][i][0] * height + y_min\n",
    "\n",
    "    return keypoints,original_keypoints\n",
    "\n",
    "def visualize_pose_static(image_path, keypoints, original_keypoints):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_original = cv2.imread(image_path)\n",
    "    keypoints = np.array(keypoints)\n",
    "    original_keypoints = np.array(original_keypoints)\n",
    "    if keypoints.shape == (1, 1, 17, 3):\n",
    "        keypoints = keypoints[0, 0]\n",
    "        # Visualizing each keypoint \n",
    "        for kp in keypoints:\n",
    "            x = int(kp[1] * image.shape[1])\n",
    "            y = int(kp[0] * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        \n",
    "        # Visualizing lines between keypoints \n",
    "        for connection in connections:\n",
    "            start_point = (int(keypoints[connection[0], 1] * image.shape[1]),\n",
    "                           int(keypoints[connection[0], 0] * image.shape[0]))\n",
    "            end_point = (int(keypoints[connection[1], 1] * image.shape[1]),\n",
    "                         int(keypoints[connection[1], 0] * image.shape[0]))\n",
    "            cv2.line(image, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        \n",
    "        # repeat with Unequalized image's predicted keypoints \n",
    "        original_keypoints = original_keypoints[0, 0]\n",
    "        for kp in original_keypoints:\n",
    "            x = int(kp[1] * image_original.shape[1])\n",
    "            y = int(kp[0] * image_original.shape[0])\n",
    "            cv2.circle(image_original, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        for connection in connections:\n",
    "            start_point = (int(original_keypoints[connection[0], 1] * image_original.shape[1]),\n",
    "                           int(original_keypoints[connection[0], 0] * image_original.shape[0]))\n",
    "            end_point = (int(original_keypoints[connection[1], 1] * image_original.shape[1]),\n",
    "                         int(original_keypoints[connection[1], 0] * image_original.shape[0]))\n",
    "            cv2.line(image_original, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        show_images([image_original,image],[\"Predicted Annotations without Processing\",\"Predicted Annotations after Processing\"])\n",
    "    else:\n",
    "        print(\"Unexpected shape of keypoints array:\", keypoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=7.39s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.41s).\n",
      "Evaluation for Equalized Images:\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.004\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=5.99s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.40s).\n",
      "Evaluation for Unequalized Images:\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.000\n"
     ]
    }
   ],
   "source": [
    "def format_coco_results(human_image_paths, predictions, coco):\n",
    "    results = []\n",
    "    \n",
    "    for image_path, pred in zip(human_image_paths, predictions):\n",
    "        img_id = int(image_path.split(\"_\")[-1].split(\".\")[0])  # Extract COCO image ID\n",
    "        keypoints = pred[0, 0, :, :]  \n",
    "        \n",
    "        # Convert normalized keypoints to absolute image coordinates\n",
    "        image = cv2.imread(image_path)\n",
    "        formatted_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            x, y, confidence = float(kp[1]), float(kp[0]), float(kp[2])\n",
    "            formatted_keypoints.extend([x, y, 2 if confidence > 0.5 else 0])  # Use confidence threshold\n",
    "        \n",
    "        results.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": 1,  # Category for 'person'\n",
    "            \"keypoints\": formatted_keypoints,\n",
    "            \"num_keypoints\": len(formatted_keypoints)/3, # undivided length is 51 since each keypoint entry has x, y, and confidence \n",
    "            \"score\": float(keypoints[:, 2].mean()) \n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Adding default values to any annotations that erroneously don't have any keypoints listed in their annotations \n",
    "for ann in coco.anns.values():\n",
    "    if 'num_keypoints' not in ann:\n",
    "        ann['num_keypoints'] = 17\n",
    "    if 'keypoints' not in ann: \n",
    "        ann['keypoints'] = []\n",
    "\n",
    "# Detect poses and format results\n",
    "predictions_equalized = []\n",
    "predictions_unequalized = []\n",
    "for static_image_path in human_image_paths:\n",
    "    annotation = [ann for ann in coco.anns.values() if ann[\"image_id\"] == int(static_image_path.split(\"_\")[-1].split(\".\")[0])]\n",
    "    static_keypoints_equalized, static_keypoints_unequalized = detect_pose_static(static_image_path,annotation)\n",
    "    predictions_equalized.append(static_keypoints_equalized)\n",
    "    predictions_unequalized.append(static_keypoints_unequalized)\n",
    "\n",
    "# Read predicted keypoints and add to json files \n",
    "coco_results = format_coco_results(human_image_paths, predictions_equalized, coco)\n",
    "results_path_equalized = \"pose_results_equalized.json\"\n",
    "with open(results_path_equalized, 'w') as f:\n",
    "    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "# Load results and evaluate (unreliable since predictions are only based on the first bounding box, of which there can be multiple)\n",
    "coco_dt = coco.loadRes(results_path_equalized)\n",
    "coco_eval = COCOeval(coco, coco_dt, \"keypoints\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "print(\"Evaluation for Equalized Images:\")\n",
    "coco_eval.summarize()\n",
    "\n",
    "# Repeat for unequalized images \n",
    "coco_results = format_coco_results(human_image_paths, predictions_unequalized, coco)\n",
    "results_path_unequalized = \"pose_results_unequalized.json\"\n",
    "with open(results_path_unequalized, 'w') as f:\n",
    "    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "coco_dt = coco.loadRes(results_path_unequalized)\n",
    "coco_eval = COCOeval(coco, coco_dt, \"keypoints\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "print(\"Evaluation for Unequalized Images:\")\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Squared Error for Unequalized Images:\n",
      "Raw Values: 71858.72607175894 \n",
      "Normalized Values: 0.12378443110708555\n",
      "Average Mean Squared Error for Equalized Images:\n",
      "Raw Values: 69068.60728587248 \n",
      "Normalized Values: 0.1201227833378025\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(lst):\n",
    "    arr = np.array(lst)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "def calculateMSE(MSE_list, human_images_anns,results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for ann in human_images_anns: \n",
    "        count = 0\n",
    "        kp = ann['keypoints']\n",
    "        for i in range(0,len(kp)-1,2):\n",
    "            x_err = kp[i] - results[count][\"keypoints\"][i]\n",
    "            y_err = kp[i+1] - results[count][\"keypoints\"][i+1]\n",
    "            MSE_list.append(x_err**2 + y_err**2)\n",
    "            count+=1  \n",
    "    MSE_list_normalized = min_max_normalize(MSE_list)\n",
    "    return np.mean(MSE_list), np.mean(MSE_list_normalized)\n",
    "\n",
    "MSE_list_equalized = []\n",
    "MSE_list_unequalized = []\n",
    "\n",
    "print(\"Average Mean Squared Error for Unequalized Images:\")\n",
    "avg_mse_unequalized, normalized_avg_mse_unequalized = calculateMSE(MSE_list_unequalized,human_images_anns,results_path_unequalized)\n",
    "print(\"Raw Values:\", avg_mse_unequalized, \"\\nNormalized Values:\",normalized_avg_mse_unequalized)\n",
    "print(\"Average Mean Squared Error for Equalized Images:\")\n",
    "avg_mse_equalized, normalized_avg_mse_equalized = calculateMSE(MSE_list_equalized,human_images_anns,results_path_equalized)\n",
    "print(\"Raw Values:\", avg_mse_equalized, \"\\nNormalized Values:\",normalized_avg_mse_equalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Absolute Error for Unequalized Images:\n",
      "Raw Values: 255.5487069720829 \n",
      "Normalized Values: 0.23715765727483196\n",
      "Average Mean Absolute Error for Equalized Images:\n",
      "Raw Values: 246.68827476774555 \n",
      "Normalized Values: 0.2300411523552192\n"
     ]
    }
   ],
   "source": [
    "def calculateMAE(MAE_list, human_images_anns,results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for ann in human_images_anns: \n",
    "        count = 0\n",
    "        kp = ann['keypoints']\n",
    "        for i in range(0,len(kp)-1,2):\n",
    "            x_err = kp[i] - results[count][\"keypoints\"][i]\n",
    "            y_err = kp[i+1] - results[count][\"keypoints\"][i+1]\n",
    "            MAE_list.append(abs(x_err) + abs(y_err))\n",
    "            count+=1  \n",
    "    MAE_list_normalized = min_max_normalize(MAE_list)\n",
    "    return np.mean(MAE_list), np.mean(MAE_list_normalized)\n",
    "\n",
    "MAE_list_equalized = []\n",
    "MAE_list_unequalized = []\n",
    "\n",
    "print(\"Average Mean Absolute Error for Unequalized Images:\")\n",
    "avg_mae_unequalized, normalized_avg_mae_unequalized = calculateMAE(MAE_list_unequalized,human_images_anns,results_path_unequalized)\n",
    "print(\"Raw Values:\", avg_mae_unequalized, \"\\nNormalized Values:\",normalized_avg_mae_unequalized)\n",
    "print(\"Average Mean Absolute Error for Equalized Images:\")\n",
    "avg_mae_equalized, normalized_avg_mae_equalized = calculateMAE(MAE_list_equalized,human_images_anns,results_path_equalized)\n",
    "print(\"Raw Values:\", avg_mae_equalized, \"\\nNormalized Values:\",normalized_avg_mae_equalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equalized_histogram(human_image_paths):\n",
    "    example_image = cv2.imread(human_image_paths[0])\n",
    "    hsv = cv2.cvtColor(example_image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    equalized_v = cv2.equalizeHist(v)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2)    \n",
    "    ax[0].hist(v.flatten(), 255)\n",
    "    ax[1].hist(equalized_v.flatten(),255)\n",
    "    ax[0].set_title(\"Unequalized Example Histogram\")\n",
    "    ax[1].set_title(\"Equalized Example Histogram\")\n",
    "\n",
    "    for a in ax:\n",
    "        a.set_xlim([0,255])\n",
    "        a.set_ylim([0,5000])\n",
    "    plt.show()    \n",
    "\n",
    "get_equalized_histogram(human_image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for static_image_path in human_image_paths:\n",
    "    print(static_image_path)\n",
    "    static_keypoints,original_static_keypoints = detect_pose_static(static_image_path)\n",
    "    visualize_pose_static(static_image_path, static_keypoints, original_static_keypoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
