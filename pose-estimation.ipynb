{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.97s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('../coco2014/annotations/instances_train2014.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "movenet = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./coco2014/images/train2014\"\n",
    "\n",
    "human_images_anns = []\n",
    "\n",
    "human_image_ids = set()\n",
    "for ann in coco.anns.values():\n",
    "    if ann['category_id'] == 1: #human\n",
    "        human_image_ids.add(ann['image_id'])\n",
    "        human_images_anns.append(ann)\n",
    "\n",
    "human_image_paths = []\n",
    "for img_id in human_image_ids:\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = f\"{image_dir}/{img_info['file_name']}\"\n",
    "    human_image_paths.append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',\n",
    "                  'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "                  'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "connections = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "               (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "def detect_pose_static(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = tf.image.resize_with_pad(tf.expand_dims(image_rgb, axis=0), 192, 192) #192 for lightning\n",
    "    image_np = image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(image_np))\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    return keypoints\n",
    "\n",
    "def visualize_pose_static(image_path, keypoints):\n",
    "    image = cv2.imread(image_path)\n",
    "    keypoints = np.array(keypoints)\n",
    "    if keypoints.shape == (1, 1, 17, 3):\n",
    "        keypoints = keypoints[0, 0]\n",
    "        for kp in keypoints:\n",
    "            x = int(kp[1] * image.shape[1])\n",
    "            y = int(kp[0] * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        for connection in connections:\n",
    "            start_point = (int(keypoints[connection[0], 1] * image.shape[1]),\n",
    "                           int(keypoints[connection[0], 0] * image.shape[0]))\n",
    "            end_point = (int(keypoints[connection[1], 1] * image.shape[1]),\n",
    "                         int(keypoints[connection[1], 0] * image.shape[0]))\n",
    "            cv2.line(image, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        show_images([image],[\"Annotated Image\"])\n",
    "    else:\n",
    "        print(\"Unexpected shape of keypoints array:\", keypoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for static_image_path in human_image_paths:\n",
    "    static_keypoints = detect_pose_static(static_image_path)\n",
    "    visualize_pose_static(static_image_path, static_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
