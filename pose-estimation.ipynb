{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('../coco2014/annotations/person_keypoints_train2014.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "movenet = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/singlepose-lightning/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../coco2014/images/train2014\"\n",
    "\n",
    "human_images_anns = []\n",
    "blacklist = [\"../coco2014/images/train2014/COCO_train2014_000000524291.jpg\",\n",
    "             \"../coco2014/images/train2014/COCO_train2014_000000262191.jpg\",\n",
    "             \"../coco2014/images/train2014/COCO_train2014_000000262171.jpg\",\n",
    "             \"../coco2014/images/train2014/COCO_train2014_000000524317.jpg\",\n",
    "             \"../coco2014/images/train2014/COCO_train2014_000000524325.jpg\",\n",
    "             \"../coco2014/images/train2014/COCO_train2014_000000000049.jpg\",\n",
    "             \"../coco2014/images/train2014/COCO_train2014_000000000061.jpg\",\n",
    "\n",
    "             ] #images that only include part of a person and should be excluded \n",
    "human_image_ids = set()\n",
    "for ann in coco.anns.values():\n",
    "    if ann['category_id'] == 1 and ann['iscrowd'] == 0: #human, not a crowd\n",
    "        human_image_ids.add(ann['image_id'])\n",
    "        human_images_anns.append(ann)\n",
    "count = 0\n",
    "human_image_paths = []\n",
    "for img_id in human_image_ids:\n",
    "    if(count < 20):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{image_dir}/{img_info['file_name']}\"\n",
    "        if(img_path not in blacklist):\n",
    "            human_image_paths.append(img_path)\n",
    "            count+=1\n",
    "\n",
    "print(len(human_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',\n",
    "                  'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "                  'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "connections = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "               (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "def detect_pose_static(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = cv2.equalizeHist(v)\n",
    "    hsv_image = cv2.merge([h, s, v])\n",
    "    final_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    image_resized = tf.image.resize_with_pad(tf.expand_dims(final_image, axis=0), 192, 192) #192 for lightning\n",
    "    image_np = image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(image_np))\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    original_image_resized = tf.image.resize_with_pad(tf.expand_dims(image, axis=0), 192, 192)\n",
    "    original_image_np = original_image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(original_image_np))\n",
    "    original_keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    return keypoints,original_keypoints\n",
    "\n",
    "def visualize_pose_static(image_path, keypoints, original_keypoints):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_original = cv2.imread(image_path)\n",
    "    keypoints = np.array(keypoints)\n",
    "    original_keypoints = np.array(original_keypoints)\n",
    "    if keypoints.shape == (1, 1, 17, 3):\n",
    "        keypoints = keypoints[0, 0]\n",
    "        for kp in keypoints:\n",
    "            x = int(kp[1] * image.shape[1])\n",
    "            y = int(kp[0] * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        for connection in connections:\n",
    "            start_point = (int(keypoints[connection[0], 1] * image.shape[1]),\n",
    "                           int(keypoints[connection[0], 0] * image.shape[0]))\n",
    "            end_point = (int(keypoints[connection[1], 1] * image.shape[1]),\n",
    "                         int(keypoints[connection[1], 0] * image.shape[0]))\n",
    "            cv2.line(image, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        \n",
    "        original_keypoints = original_keypoints[0, 0]\n",
    "        for kp in original_keypoints:\n",
    "            x = int(kp[1] * image_original.shape[1])\n",
    "            y = int(kp[0] * image_original.shape[0])\n",
    "            cv2.circle(image_original, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        for connection in connections:\n",
    "            start_point = (int(original_keypoints[connection[0], 1] * image_original.shape[1]),\n",
    "                           int(original_keypoints[connection[0], 0] * image_original.shape[0]))\n",
    "            end_point = (int(original_keypoints[connection[1], 1] * image_original.shape[1]),\n",
    "                         int(original_keypoints[connection[1], 0] * image_original.shape[0]))\n",
    "            cv2.line(image_original, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        show_images([image_original,image],[\"Predicted Annotations without Processing\",\"Predicted Annotations after Processing\"])\n",
    "    else:\n",
    "        print(\"Unexpected shape of keypoints array:\", keypoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../coco2014/images/train2014/COCO_train2014_000000262145.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000262146.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393223.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393224.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000524297.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393227.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000131084.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393230.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000524311.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393241.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000524314.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000131101.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000524320.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393251.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000000036.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000524338.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393268.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000131127.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000262207.jpg\n",
      "../coco2014/images/train2014/COCO_train2014_000000393290.jpg\n"
     ]
    }
   ],
   "source": [
    "for static_image_path in human_image_paths:\n",
    "    print(static_image_path)\n",
    "    static_keypoints,original_static_keypoints = detect_pose_static(static_image_path)\n",
    "    #visualize_pose_static(static_image_path, static_keypoints, original_static_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Unequalized Images\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=4.92s).\n",
      "Evaluation for Unequalized Images:\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.51s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.000\n"
     ]
    }
   ],
   "source": [
    "def format_coco_results(human_image_paths, predictions, coco):\n",
    "    results = []\n",
    "    for image_path, pred in zip(human_image_paths, predictions):\n",
    "        img_id = int(image_path.split(\"_\")[-1].split(\".\")[0])  # Extract COCO image ID\n",
    "        keypoints = pred[0, 0, :, :]  # Shape: [17, 3]\n",
    "        \n",
    "        # Convert normalized keypoints to absolute image coordinates\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, _ = image.shape\n",
    "        formatted_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            x, y, confidence = float(kp[1] * w), float(kp[0] * h), float(kp[2])\n",
    "            formatted_keypoints.extend([x, y, confidence])  # Use confidence threshold\n",
    "        \n",
    "        results.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": 1,  # Category for 'person'\n",
    "            \"keypoints\": formatted_keypoints,\n",
    "            \"num_keypoints\": 17,\n",
    "            \"score\": float(keypoints[:, 2].mean())\n",
    "        })\n",
    "    return results\n",
    "\n",
    "for ann in coco.anns.values():\n",
    "    if 'num_keypoints' not in ann:\n",
    "        ann['num_keypoints'] = 17\n",
    "    if 'keypoints' not in ann: \n",
    "        ann['keypoints'] = []\n",
    "\n",
    "# Detect poses and format results\n",
    "predictions = []\n",
    "for static_image_path in human_image_paths:\n",
    "    static_keypoints, _ = detect_pose_static(static_image_path)\n",
    "    predictions.append(static_keypoints)\n",
    "\n",
    "coco_results = format_coco_results(human_image_paths, predictions, coco)\n",
    "results_path = \"pose_results_equalized.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "# Load results and evaluate\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "for static_image_path in human_image_paths:\n",
    "    _, static_keypoints = detect_pose_static(static_image_path)\n",
    "    predictions.append(static_keypoints)\n",
    "\n",
    "coco_results = format_coco_results(human_image_paths, predictions, coco)\n",
    "results_path = \"pose_results_unequalized.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation for Unequalized Images\")\n",
    "coco_dt = coco.loadRes(results_path)\n",
    "coco_eval = COCOeval(coco, coco_dt, \"keypoints\")\n",
    "coco_eval.evaluate()\n",
    "print(\"Evaluation for Unequalized Images:\")\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
