{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(titles[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.93s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO('../coco2014/annotations/person_keypoints_train2014.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\austi\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movenet = hub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/singlepose-lightning/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_full_body(ann, min_keypoints=17):\n",
    "    keypoints = ann['keypoints']\n",
    "    num_visible = sum(1 for i in range(0, len(keypoints)) if keypoints[i] > 0)  # Count visible keypoints\n",
    "    if num_visible < min_keypoints:\n",
    "        return False  # Not all keypoints are visible\n",
    "    \n",
    "    bbox = ann['bbox']\n",
    "    bbox_aspect_ratio = bbox[3] / bbox[2]  \n",
    "    if bbox_aspect_ratio < 1.5:  # Threshold for a standing full-body shot\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"../coco2014/images/train2014\"\n",
    "\n",
    "human_images_anns = []\n",
    "\n",
    "human_image_ids = set()\n",
    "for ann in coco.anns.values():\n",
    "    if ann['category_id'] == 1 and ann['iscrowd'] == 0 and is_full_body(ann,17) == True: #human, not a crowd, displays full body\n",
    "        human_image_ids.add(ann['image_id'])\n",
    "        human_images_anns.append(ann)\n",
    "count = 0\n",
    "human_image_paths = []\n",
    "for img_id in human_image_ids:\n",
    "    if(count < 200):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = f\"{image_dir}/{img_info['file_name']}\"\n",
    "        human_image_paths.append(img_path)\n",
    "        count+=1\n",
    "\n",
    "print(len(human_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_names = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',\n",
    "                  'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
    "                  'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "connections = [(0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "               (5, 6), (5, 11), (6, 12), (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "def detect_pose_static(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = cv2.medianBlur(v,3)\n",
    "    v = cv2.equalizeHist(v)\n",
    "    hsv_image = cv2.merge([h, s, v])\n",
    "    final_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    image_resized = tf.image.resize_with_pad(tf.expand_dims(final_image, axis=0), 192, 192) #192 for lightning\n",
    "    image_np = image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(image_np))\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    original_image_resized = tf.image.resize_with_pad(tf.expand_dims(image, axis=0), 192, 192)\n",
    "    original_image_np = original_image_resized.numpy().astype(np.int32)\n",
    "    outputs = movenet.signatures[\"serving_default\"](tf.constant(original_image_np))\n",
    "    original_keypoints = outputs['output_0'].numpy()\n",
    "\n",
    "    return keypoints,original_keypoints\n",
    "\n",
    "def visualize_pose_static(image_path, keypoints, original_keypoints):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_original = cv2.imread(image_path)\n",
    "    keypoints = np.array(keypoints)\n",
    "    original_keypoints = np.array(original_keypoints)\n",
    "    if keypoints.shape == (1, 1, 17, 3):\n",
    "        keypoints = keypoints[0, 0]\n",
    "        for kp in keypoints:\n",
    "            x = int(kp[1] * image.shape[1])\n",
    "            y = int(kp[0] * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        for connection in connections:\n",
    "            start_point = (int(keypoints[connection[0], 1] * image.shape[1]),\n",
    "                           int(keypoints[connection[0], 0] * image.shape[0]))\n",
    "            end_point = (int(keypoints[connection[1], 1] * image.shape[1]),\n",
    "                         int(keypoints[connection[1], 0] * image.shape[0]))\n",
    "            cv2.line(image, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        \n",
    "        original_keypoints = original_keypoints[0, 0]\n",
    "        for kp in original_keypoints:\n",
    "            x = int(kp[1] * image_original.shape[1])\n",
    "            y = int(kp[0] * image_original.shape[0])\n",
    "            cv2.circle(image_original, (x, y), 12, (255, 0, 0), -1)  # Increase thickness and change color to blue\n",
    "        for connection in connections:\n",
    "            start_point = (int(original_keypoints[connection[0], 1] * image_original.shape[1]),\n",
    "                           int(original_keypoints[connection[0], 0] * image_original.shape[0]))\n",
    "            end_point = (int(original_keypoints[connection[1], 1] * image_original.shape[1]),\n",
    "                         int(original_keypoints[connection[1], 0] * image_original.shape[0]))\n",
    "            cv2.line(image_original, start_point, end_point, (0, 0, 255), 8)  # Increase thickness and change color to red\n",
    "        show_images([image_original,image],[\"Predicted Annotations without Processing\",\"Predicted Annotations after Processing\"])\n",
    "    else:\n",
    "        print(\"Unexpected shape of keypoints array:\", keypoints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_coco_results(human_image_paths, predictions, coco):\n",
    "    results = []\n",
    "    for image_path, pred in zip(human_image_paths, predictions):\n",
    "        img_id = int(image_path.split(\"_\")[-1].split(\".\")[0])  # Extract COCO image ID\n",
    "        keypoints = pred[0, 0, :, :]  # Shape: [17, 3]\n",
    "        \n",
    "        # Convert normalized keypoints to absolute image coordinates\n",
    "        image = cv2.imread(image_path)\n",
    "        h, w, _ = image.shape\n",
    "        formatted_keypoints = []\n",
    "        for kp in keypoints:\n",
    "            x, y, confidence = float(kp[1] * w), float(kp[0] * h), float(kp[2])\n",
    "            formatted_keypoints.extend([x, y, 2 if confidence > 0.3 else 0])  # Use confidence threshold\n",
    "        \n",
    "        results.append({\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": 1,  # Category for 'person'\n",
    "            \"keypoints\": formatted_keypoints,\n",
    "            \"num_keypoints\": len(formatted_keypoints)/3,\n",
    "            \"score\": float(keypoints[:, 2].mean())\n",
    "        })\n",
    "    return results\n",
    "\n",
    "for ann in coco.anns.values():\n",
    "    if 'num_keypoints' not in ann:\n",
    "        ann['num_keypoints'] = 17\n",
    "    if 'keypoints' not in ann: \n",
    "        ann['keypoints'] = []\n",
    "\n",
    "# Detect poses and format results\n",
    "predictions_equalized = []\n",
    "predictions_unequalized = []\n",
    "for static_image_path in human_image_paths:\n",
    "    static_keypoints_equalized, static_keypoints_unequalized = detect_pose_static(static_image_path)\n",
    "    predictions_equalized.append(static_keypoints_equalized)\n",
    "    predictions_unequalized.append(static_keypoints_unequalized)\n",
    "\n",
    "\n",
    "coco_results = format_coco_results(human_image_paths, predictions_equalized, coco)\n",
    "results_path_equalized = \"pose_results_equalized.json\"\n",
    "with open(results_path_equalized, 'w') as f:\n",
    "    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "# Load results and evaluate\n",
    "coco_dt = coco.loadRes(results_path_equalized)\n",
    "coco_eval = COCOeval(coco, coco_dt, \"keypoints\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "print(\"Evaluation for Equalized Images:\")\n",
    "coco_eval.summarize()\n",
    "\n",
    "coco_results = format_coco_results(human_image_paths, predictions_unequalized, coco)\n",
    "results_path_unequalized = \"pose_results_unequalized.json\"\n",
    "with open(results_path_unequalized, 'w') as f:\n",
    "    json.dump(coco_results, f, indent=2)\n",
    "\n",
    "coco_dt = coco.loadRes(results_path_unequalized)\n",
    "coco_eval = COCOeval(coco, coco_dt, \"keypoints\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "print(\"Evaluation for Unequalized Images:\")\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Squared Error for Unqualized Images:\n",
      "0.13982700206037207\n",
      "Average Mean Squared Error for Equalized Images:\n",
      "0.1459409160480021\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(lst):\n",
    "    arr = np.array(lst)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min())\n",
    "\n",
    "def calculateMSE(MSE_list, human_images_anns,results_path):\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    for ann in human_images_anns: \n",
    "        count = 0\n",
    "        kp = ann['keypoints']\n",
    "        for i in range(0,len(kp)-1,2):\n",
    "            x_err = kp[i] - results[count][\"keypoints\"][i]\n",
    "            y_err = kp[i+1] - results[count][\"keypoints\"][i+1]\n",
    "            MSE_list.append(x_err**2 + y_err**2)\n",
    "            count+=1  \n",
    "    MSE_list = min_max_normalize(MSE_list)\n",
    "    return np.mean(MSE_list)\n",
    "\n",
    "MSE_list_equalized = []\n",
    "MSE_list_unequalized = []\n",
    "\n",
    "print(\"Average Mean Squared Error for Unequalized Images:\")\n",
    "print(calculateMSE(MSE_list_unequalized,human_images_anns,results_path_unequalized))\n",
    "print(\"Average Mean Squared Error for Equalized Images:\")\n",
    "print(calculateMSE(MSE_list_equalized,human_images_anns,results_path_equalized))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for static_image_path in human_image_paths:\n",
    "    #print(static_image_path)\n",
    "    #static_keypoints,original_static_keypoints = detect_pose_static(static_image_path)\n",
    "    #visualize_pose_static(static_image_path, static_keypoints, original_static_keypoints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
